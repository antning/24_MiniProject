{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea85d164-b542-495c-8526-39ed2ff24701",
   "metadata": {},
   "source": [
    "## Calculation of Term Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03ee254-7530-4da3-840f-f9c80fed04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Load_MasterDictionary as lm\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3083b37b-cf8d-41ab-b87d-ccec3015d8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...Loading Master Dictionary 85000\n",
      "Master Dictionary loaded from file: \n",
      "  /Users/anthony_ning/NYU/2024Internship/EDGAR/LoughranMcDonald_MasterDictionary_2014.csv\n",
      "  85,131 words loaded in master_dictionary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in master dictionary\n",
    "dic_path = '/Users/anthony_ning/NYU/2024Internship/EDGAR/LoughranMcDonald_MasterDictionary_2014.csv'\n",
    "md = lm.load_masterdictionary(dic_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b3330-ba05-477b-b189-be07b2bef7ba",
   "metadata": {},
   "source": [
    "### tf.idf Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17f18ec-2cdd-41ff-a305-7590805292d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list containing only negative words\n",
    "\n",
    "wrd_lst = []\n",
    "\n",
    "for word in md:\n",
    "    if md[word].sentiment['negative']:\n",
    "        wrd_lst.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc1010-d43e-43af-9861-aa0af367d837",
   "metadata": {},
   "source": [
    "#### Calculation of the 1st matrix: the raw count of each negative word in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0366278a-89fd-47cf-b6e6-2f52b0f4ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first matrix\n",
    "total_negwrd = len(wrd_lst)\n",
    "\n",
    "def get_matrix1(doc):\n",
    "    \n",
    "    # The occurence of each negative word in txt file\n",
    "    _o_neg = [0] * total_negwrd\n",
    "\n",
    "    tokens = re.findall('\\w+', doc)  # Note that \\w+ splits hyphenated words\n",
    "    \n",
    "    for token in tokens:\n",
    "            \n",
    "        if not token.isdigit() and len(token) > 1 and token in wrd_lst:\n",
    "            _o_neg[wrd_lst.index(token)] += 1\n",
    "\n",
    "    return np.array(_o_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87948b1b-d661-4097-b2cf-f349d7d0c519",
   "metadata": {},
   "source": [
    "#### Calculation of the 2nd matrix: the number of documents containing at least one occurance of each negative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bde1dbd-839e-4c14-816e-03182f610c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second matrix\n",
    "def get_matrix2(doc):\n",
    "    \n",
    "    # The occurence of each negative word in txt file\n",
    "    _o_doc = [0] * total_negwrd\n",
    "\n",
    "    tokens = re.findall('\\w+', doc)  # Note that \\w+ splits hyphenated words\n",
    "    \n",
    "    for token in tokens:\n",
    "            \n",
    "        if not token.isdigit() and len(token) > 1 and token in wrd_lst:\n",
    "            _o_doc[wrd_lst.index(token)] = 1\n",
    "\n",
    "    return np.array(_o_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb56b4-4022-461a-b861-e92b7a282fd8",
   "metadata": {},
   "source": [
    "#### Calculation of the 3rd matrix: the total word count in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02333769-4642-4271-93ee-0677047c493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third matrix\n",
    "def get_matrix3(doc):\n",
    "    \n",
    "    _o_total = 0\n",
    "\n",
    "    tokens = re.findall('\\w+', doc)  # Note that \\w+ splits hyphenated words\n",
    "    \n",
    "    for token in tokens:\n",
    "            \n",
    "        if not token.isdigit() and len(token) > 1 and token in md:\n",
    "            _o_total += 1\n",
    "\n",
    "    return _o_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d61ec7b-99fe-48e3-afeb-862f4a49e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the three matrices for 10-K and 10-Q files\n",
    "TARGET_FILES = './txt/*.txt'\n",
    "\n",
    "file_list = glob.glob(TARGET_FILES)\n",
    "\n",
    "matrix_1 = np.zeros((len(file_list), total_negwrd))\n",
    "matrix_2 = np.zeros((len(file_list), total_negwrd))\n",
    "matrix_3 = np.zeros((len(file_list), 1))\n",
    "\n",
    "orders = []\n",
    "i = 0\n",
    "\n",
    "for filename in file_list:\n",
    "    \n",
    "    orders.append(filename)\n",
    "    \n",
    "    with open(filename, 'r', encoding='UTF-8', errors='ignore') as f_in:\n",
    "        doc = f_in.read()\n",
    "\n",
    "    doc = doc.upper()\n",
    "    \n",
    "    output_matrix1 = get_matrix1(doc)\n",
    "    matrix_1[i] = output_matrix1\n",
    "    output_matrix2 = get_matrix2(doc)\n",
    "    matrix_2[i] = output_matrix2\n",
    "    t_wrd = get_matrix3(doc)\n",
    "    matrix_3[i] = t_wrd\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "matrix_2 = np.sum(matrix_2, axis=0)\n",
    "matrix_1 = matrix_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee4da2-9497-4166-8e4f-cc8d4c8559d7",
   "metadata": {},
   "source": [
    "#### tf.idf calculation (please refer to the formula in the paper of Loughran and McDonald's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712a042a-9a25-4044-ac58-22289fda41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/75vh_thj2m57nynzg5tgw0300000gn/T/ipykernel_61396/1915819114.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  wrd_weight[i][j] = (np.log(matrix_1[i][j])+1)/(np.log(matrix_3[j])+1)*np.log(N/matrix_2[i])\n"
     ]
    }
   ],
   "source": [
    "N = len(file_list)\n",
    "wrd_weight = np.zeros((total_negwrd, N))\n",
    "\n",
    "for i in range(total_negwrd):\n",
    "    for j in range(N):\n",
    "        if matrix_1[i][j] == 0:\n",
    "            wrd_weight[i][j] = 0\n",
    "        else:\n",
    "            wrd_weight[i][j] = (np.log(matrix_1[i][j])+1)/(np.log(matrix_3[j])+1)*np.log(N/matrix_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ca4e3-1912-4519-b9fe-d203e85a2cf2",
   "metadata": {},
   "source": [
    "#### Term weights export to .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02ad3ac-53cc-4a87-9fb1-77ce765945d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined output file\n",
    "OUTPUT_FILE = './sp_testfile_weight.csv'\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    for i in range(N+1):\n",
    "        if i == 0:\n",
    "            f.write(' ,')\n",
    "        elif i== N:\n",
    "            f.write(orders[i-1]+'\\n')\n",
    "        else:\n",
    "            f.write(orders[i-1]+',')\n",
    "    \n",
    "    for i in range(total_negwrd):\n",
    "        f.write(wrd_lst[i]+',')\n",
    "        for j in range(N):\n",
    "            if j != N-1:\n",
    "                f.write(str(wrd_weight[i][j])+',')\n",
    "            else:\n",
    "                f.write(str(wrd_weight[i][j])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283705d2-e29a-435f-bfba-7006a2c7b562",
   "metadata": {},
   "source": [
    "### Proportional Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa43263-32d0-4fa5-b324-cc4343089415",
   "metadata": {},
   "source": [
    "According to the paper of Loughran and McDonald's, the proportional weights are the word list counts relative to the total number of words appearing in txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57363126-a90d-461c-b3b8-f9fb85aed01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(doc):\n",
    "    \n",
    "    # The occurence of each negative word in txt file\n",
    "    _percentage_neg = [0] * total_negwrd\n",
    "\n",
    "    # The total word count\n",
    "    total_wrd = 0\n",
    "\n",
    "    tokens = re.findall('\\w+', doc)  # Note that \\w+ splits hyphenated words\n",
    "    \n",
    "    for token in tokens:\n",
    "\n",
    "        if not token.isdigit() and len(token) > 1 and token in md:\n",
    "            total_wrd += 1\n",
    "            \n",
    "        if not token.isdigit() and len(token) > 1 and token in wrd_lst:\n",
    "            _percentage_neg[wrd_lst.index(token)] += 1\n",
    "\n",
    "    return np.array(_percentage_neg)/total_wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447430ff-d8ee-4810-8017-e479df8d87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportional weights for each 10-K and 10-Q file\n",
    "pro_weights = np.zeros((N, total_negwrd))\n",
    "pro_orders = []\n",
    "i = 0\n",
    "\n",
    "for filename in file_list:\n",
    "    \n",
    "    pro_orders.append(filename)\n",
    "    \n",
    "    with open(filename, 'r', encoding='UTF-8', errors='ignore') as f_in:\n",
    "        doc = f_in.read()\n",
    "\n",
    "    doc = doc.upper()\n",
    "    pro_weights[i] = get_weight(doc)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "pro_weights = pro_weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4303c7f8-8876-44e7-ba9f-c1b9e26d8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined output file\n",
    "OUTPUT_FILE = './sp_testfile_proweight.csv'\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    \n",
    "    for i in range(N+1):\n",
    "        if i == 0:\n",
    "            f.write(' ,')\n",
    "        elif i== N:\n",
    "            f.write(pro_orders[i-1]+'\\n')\n",
    "        else:\n",
    "            f.write(pro_orders[i-1]+',')\n",
    "    \n",
    "    for i in range(total_negwrd):\n",
    "        f.write(wrd_lst[i]+',')\n",
    "        for j in range(N):\n",
    "            if j != N-1:\n",
    "                f.write(str(pro_weights[i][j])+',')\n",
    "            else:\n",
    "                f.write(str(pro_weights[i][j])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455ac2a-8387-4637-8e6c-7f4d52604fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
